{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Notes and Data Science Project Template\n\n"
  },
  {
   "metadata": {
    "trusted": true,
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "cell_type": "code",
   "source": "%pip install --upgrade scikit-learn \n# Did this to use latest regressors from sklearn...",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /Users/tomcounsell/.virtualenvs/AI/lib/python3.6/site-packages (0.23.2)\r\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /Users/tomcounsell/.virtualenvs/AI/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\r\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/tomcounsell/.virtualenvs/AI/lib/python3.6/site-packages (from scikit-learn) (0.16.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/tomcounsell/.virtualenvs/AI/lib/python3.6/site-packages (from scikit-learn) (1.19.1)\r\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/tomcounsell/.virtualenvs/AI/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "import os\nfrom datetime import datetime\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\nimport seaborn as sns\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-98908bd4d3de>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mskew\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mboxcox_normmax\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspecial\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mboxcox1p\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_columns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m250\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inputs\n\n1. Load the data\n2. Taking a look at it\n3. Tidy up and prepare training and labeling"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Loading datasets.\n\ndata_train = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ndata_test = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\ndata_train_original, data_test_original = data_train, data_test\nprint(f\"\"\"\ndata_train.shape == {data_train.shape}\ndata_test.shape == {data_test.shape}\n\"\"\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "## Inspect the data and come back anytime to review\n\ndata_train.head()\n# data_test.head()\n# data_train.describe()\n# data_test.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Drop useless columns\n\n- `Id` column can be safely dropped from both. \n- Also save the prediction target `SalePrice` on a new variable\n"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Drop Id column\ndata_train.drop('Id', axis=1, inplace=True)\ndata_test.drop('Id', axis=1, inplace=True)\n\n# Drop target from data_train\ndata_train.drop(['SalePrice'], axis=1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set y target"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "y = data_train['SalePrice'].reset_index(drop=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis\n\nPlay with the data to better understand it. Possibly, we can do some simple transformations (adding our own human wisdom) to make things easier for the ML architecture\n\n\n### basic correlation table\n\nWith this table we can understand some linear relations between different features.\n"
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "\n# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_data_train = data_train.corr()\nmask = np.triu(correlation_data_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Observations:\n- There's strong relation between overall quality of the houses and their sale prices.\n- Again above grade living area seems strong indicator for sale price.\n- Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n- There are some obvious relations we can possibly merge like (total sqft : num rooms) and (num cars : garage area)\n- Overall condition of the house is NOT directly correlated with pricing, it's interesting and worth digging.\n\n- **I'm going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Merging train test features for engineering.\n\nfeatures = pd.concat([data_train, data_test]).reset_index(drop=True)\nprint(features.shape)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missing Data\n- looking at the data description, we can see that most of these missing data actually not missing\n- many simply mean that the house doesn't have that specific feature\n\n1. list visualize our missing values\n2. remove, fill, or substitute them\n"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def missing_percentage(df):\n    \n    \"\"\"\n    A function for returning missing ratios.\n    \"\"\"\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) / len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) / len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# Checking 'NaN' values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How to fix missing data:\n\n\n1. Replace `\"NaN\"` with `None` for columns of non-measured values\n1. Replace `\"NaN\"` with `0` for columns of measured values\n2. Replace `\"NaN\"` with the mode for columns where `None` or `0` would be too harmful\n2. If possible, use other human insights to replace labels with a proxy or a new custom label"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Columns where 'NaN' means None\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\n# Columns where 'NaN' means 0\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\n# Columns where 'NaN' can be replaced with mode\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)\n\n    \n# Filling 'MSZoning' according to MSSubClass.\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\n\n# Filling 'MSZoning' according to Neighborhood.\nfeatures['LotFrontage'] = features.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\n# Features which numerical on data but should be treated as category:\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Engineering\n\nGroup columns where values are sparse, merge them to a 2nd class column (eg. \"other\")"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Transforming rare values(less than 10) into one group.\nother_cols = [\n    'Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n    'Heating', 'Electrical', 'Functional', 'SaleType'\n]\n\nfor col in other_cols:\n    mask = features[col].isin(\n        features[col].value_counts()[features[col].value_counts() < 10].index)\n    features[col][mask] = 'Other'",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Categorical Data\n\nSome numerical features show us correlation on the heatmap, but not categorical values.\nFor relations between categorical data and the target (SalePrice), Boxplots help\nSort them by the median value of the group to see the importances in descending order.\n"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Displaying sale prices vs. categorical values:\ndef srt_box(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()\n\nsrt_box('SalePrice', data_train)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Observations:\n\n- **MSZoning;**\n - Floating village houses (I assume they are some kind of special area that retired community resides, has the highest median value.\n - Residental low density houses comes second with the some outliers.\n - Residental high and low seems similar meanwhile commercial is the lowest.\n\n- **LandContour;**\n - Hillside houses seems a little bit higher expensive than the rest meanwhile banked houses are the lowest.\n\n- **Neighborhood;**\n - Northridge Heights, Northridge and Timberland are top 3 expensive places for houses.\n - Somerset, Veenker, Crawford, Clear Creek, College Creek and Bloomington Heights seems above average.\n - Sawyer West has wide range for prices related to similar priced regions.\n - Old Town and Edwards has some outlier prices but they generally below average.\n - Briardale, Iowa DOT and Rail Road, Meadow Village are the cheapest places for houses it seems...\n\n- **Conditions;**\n - Meanwhile having wide range of values being close to North-South Railroad seems having positive effect on the price.\n - Being near or adjacent to positive off-site feature (park, greenbelt, etc.) increases the price.\n - These values are pretty similar but we can get some useful information from them.\n \n- **MasVnrType;** Having stone masonry veneer seems better priced than having brick.\n\n- **Quality Features;** There are many categorical quality values that affects the pricing on some degree, we're going to quantify them so we can create new features based on them. So we don't dive deep on them in this part.\n\n- **CentralAir;** Having central air system has decent positive effect on sale prices.\n\n- **GarageType;** \n  - Built-In (Garage part of house - typically has room above garage) garage typed houses are the most expensive ones.\n  - Attached garage types following the built-in ones.\n  - Car ports are the lowest\n  \n- **Misc;** Sale type has some kind of effect on the prices but we won't get into details here. Btw... It seems having tennis court is really adding price to your house, who would have known :)\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Upon inspecting the categorical data, convert some of these categories to numerical ones\nespecially where the target (SalePrice) seems strongly related to specific features"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Converting some of the categorical values to numeric ones.\n\nneigh_map = {\n    'MeadowV': 1, 'IDOTRR': 1, 'BrDale': 1,\n    'BrkSide': 2, 'OldTown': 2, 'Edwards': 2,\n    'Sawyer': 3, 'Blueste': 3, 'SWISU': 3, 'NPkVill': 3, 'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5, 'NWAmes': 5, 'Gilbert': 5, 'Blmngtn': 5,\n    'CollgCr': 6, 'ClearCr': 6, 'Crawfor': 6,\n    'Veenker': 7, 'Somerst': 7,\n    'Timber': 8, 'StoneBr': 9, 'NridgHt': 10, 'NoRidge': 10\n}\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype(\n    'int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6\n}\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Numeric Data\n\nA Scatter Plot is useful to see how numeric features affect the target (SalePrice)\nAdd a polynomial regression line to see a general trend line. \nTry to understand the numerical values and their significance. Also, spot outliers.\n"
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Plotting numerical features with polynomial order to detect outliers.\n\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\nsrt_reg('SalePrice', data_train)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Observations:\n\n- **OverallQual;** It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n- **OverallCondition;** Looks like overall condition is left skewed where most of the houses are around 5/10 condition. But it doesn't effect the price like quality indicator...\n\n- **YearBuilt;** Again new buildings are generally expensive than the old ones.\n\n- **Basement;** General table shows bigger basements are increasing the price but I see some outliers there...\n\n- **GrLivArea;** This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n- **SaleDates;** They seem pretty unimportant on sale prices, we can drop them..."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drop Outliers\n\nDrop some outliers we detected them just above.\nThis part is subjective and you can try different approaches."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Dropping outliers after detecting them by eye.\nfeatures = features.drop(features[(features['OverallQual'] < 5) & (features['SalePrice'] > 200000)].index)\n# ...\n\nfeatures.drop(columns='SalePrice', inplace=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating New Features\n\nOk in this part we going to create some features, these can improve our modelling. I went with basic approach by merging some important indicators and making them stronger."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Creating new features  based on previous observations. There might be some highly correlated features now. Drop them if you want to...\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Observing the effects of newly created features on sale price.\n\ndef srt_reg(feature):\n    merged = features.join(y)\n    fig, axes = plt.subplots(5, 3, figsize=(25, 40))\n    axes = axes.flatten()\n\n    new_features = [\n        'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm',\n        'TotalExtQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr',\n        'QualBsm', 'QualPorch', 'QualExt', 'QualGrg', 'QlLivArea', 'QualSFNg'\n    ]\n\n    for i, j in zip(new_features, axes):\n\n        sns.regplot(x=i,\n                    y=feature,\n                    data=merged,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\nsrt_reg('SalePrice')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Checking New Features\n\nWell... They look decent enough, I hope these can help us building strong models. I also wanted to add some more basic features for having specific feature or not. This approach was widely accepted by community so I see no harm to add them."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Creating some simple features.\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transforming the Data\n\nSome of the continious values are not distributed evenly and not fitting on normal distribution, we can fix them by using couple transformation approaches. We're going to use boxcox here, again it's widely used by community and I want to thank them all for their great work. \n\nWe're going to list skewed features and then apply boxcox transformation with boxcox_normmax (It computes optimal boxcox transform parameter for input data, so we don't decide the lambda here)..."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Numerical features we worked on which seems highly skewed but we filter again anyways...\n\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Finding skewness of the numerical features.\n\nskew_features = np.abs(features[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Here we dropping some unnecessary features had their use in feature engineering or not needed at all. Obviously it's subjective but I feel they don't add much to model. Then we one hot encode the categorical data left so everything will be prepared for the modelling.**"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Features to drop:\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n# Dropping features.\n\nfeatures.drop(columns=to_drop, inplace=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Getting dummy variables for categorical data.\n\nfeatures = pd.get_dummies(data=features)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Double Check\n\n- **Before we move to modelling I want to take one last look to the data we processed. Everyting seems in order, not missing datas, values are numerical etc. Our feature engineered data is present...**\n\n- **Just want to check how transformed data correlates with sale prices before we move on and it looks decent.**\n\n- **Again I wanted to check our target value distribution and it seems little skewed. We can fix this by applying log transformation so our models can perform better.**"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "print(f'Number of missing values: {features.isna().sum().sum()}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "features.shape",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "features.sample(5)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "features.describe()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Separating train and test set.\n\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "#merged=data_train.join(y)\n\ncorrelations = data_train.join(y).corrwith(data_train.join(y)['SalePrice']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.5], cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "cell_type": "code",
   "source": "def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Checking target variable.\n\nplot_dist3(data_train.join(y), 'SalePrice', 'Sale Price Before Log Transformation')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Setting model data.\n\nX = train\nX_test = test\ny = np.log1p(y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "plot_dist3(data_train.join(y), 'SalePrice', 'Sale Price After Log Transformation')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modelling\n\nWell then, it's time to do some modelling! First of all I wanted to thank kaggle community for loads of examples inspired me. Especially Alex Lekov's great script and Serigne's stacked regressions approach were great guides for me!\n\nLet's start with loading packages needed and then we set our regressors. The regressors I'm going to use here are:\n\n- Ridge,\n- Lasso,\n- Elasticnet,\n- Support Vector Regression\n + I'm going to apply robust scaler on these before we run them because they really get effected by outliers.\n- Gradient Boosting Regressor\n- LightGBM Regressor\n- XGBoost Regressor\n + These don't need scaling in my opinion so we just go as it is\n- Hist Gradient Boosting Regressor\n + This is just for experimenting, it's still experimental on sklearn anyways\n- Tweedie Regressor\n + This regressor added in latest version of sklearn and I wanted to try it. It's generalized linear model with a Tweedie distribution. We gonna use power of 0 because we expecting normal target distribution but you can try this or other generalized models like poisson regressor or gamma regressor.\n\nI tried to tune models by using Optuna package, that part is not added here."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part works well with sklearn and others...",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "alphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n\n# hist gradient boosting regressor\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth= 2,\n    min_samples_leaf= 40,\n    max_leaf_nodes= 29,\n    learning_rate= 0.15,\n    max_iter= 225,\n                                    random_state=42)\n\n# tweedie regressor\n \ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross Validation"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Results\n\nAllright, our results are here. Looks like our models did pretty close to each other, there might be some overfitting models and we can try to fix them by tuning but it was computationally expensive for me and since I'm going to stack and blend the models I think we can leave them as it is. We already added our models to stacking regression and set the XGBoost as meta regressor we can continue with stacking"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stacking & Blending\n\nHere we fit every single estimator we have on the train data and then blend them by assigning weights to each model and sum the results. Weights are pretty subjective and I'm pretty sure you can come up with something performs better than this if you play with it..."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Blending models by assigning weights:\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.05 * hist_full_data.predict(X)) +\n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Submission\n\nOur models are tuned, stacked, fitted and blended so we are ready to predict and submit our results. One last thing that I have seen on couple examples adding weights on some quantile levels. It didn't increase my results a lot but still improved the end results a little so I decided to use it."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "submission = pd.read_csv('/kaggle/input/home-data-for-ml-course/data_test.csv')\n# Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n# Defining outlier quartile ranges\nq1 = submission['SalePrice'].quantile(0.0050)\nq2 = submission['SalePrice'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\nsubmission['SalePrice'] = submission['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)\nsubmission = submission[['Id', 'SalePrice']]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "submission.to_csv('mysubmission.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)\nsubmission.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Credit\n\nBig thanks to [Ertuğrul Demir](https://www.kaggle.com/datafan07) for providing the shoulders to stand on. This notebook started as a fork of [My Top 1% Approach: EDA, New Models and Stacking](https://www.kaggle.com/datafan07/my-top-1-approach-eda-new-models-and-stacking)\n\n-----\n\n#### My main objectives on this project are:\n+ Applying exploratory data analysis and trying to get some insights about our dataset\n+ Getting data in better shape by transforming and feature engineering to help us in building better models\n+ Building and tuning couple models to get some stable results on predicting housing prices\n\n#### In this notebook we are going to try explore the data we have and going try answer questions like:\n+ What are the main predictors for house pricing?\n+ What is more important on pricing, having big area for housing or just being in better neighborhood?\n+ Is quality of the house alone more important than having nice garages or basements?\n+ There are some features that can be modified and depends on the building but there are some other features like cannot be changed like location of the house, which group is effecting house prices?\n+ Can we predict the price of a house with the given traning data using machine learning techniques.\n+ What can our predictions achieve with different approaches?\n+ If we stack and blend the models, can we get more regularized results?\n\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}