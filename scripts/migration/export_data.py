#!/usr/bin/env python3
"""
Data Export Tool for AI Rebuild Migration
Exports existing data including chat history, preferences, workspaces, tool metrics, and system state.
"""

import json
import sqlite3
import os
import shutil
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

@dataclass
class ExportConfig:
    """Configuration for data export"""
    source_db_path: str
    export_dir: str
    include_chat_history: bool = True
    include_preferences: bool = True
    include_workspaces: bool = True
    include_tool_metrics: bool = True
    include_system_state: bool = True
    compress_output: bool = True
    encryption_key: Optional[str] = None

class DataExporter:
    """Main data export handler"""
    
    def __init__(self, config: ExportConfig):
        self.config = config
        self.export_dir = Path(config.export_dir)
        self.export_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.export_dir / 'export.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def export_all_data(self) -> Dict[str, Any]:
        """Export all data based on configuration"""
        self.logger.info("Starting data export process")
        
        export_manifest = {
            'export_timestamp': datetime.utcnow().isoformat(),
            'export_version': '1.0.0',
            'source_system': 'ai-rebuild-legacy',
            'target_system': 'ai-rebuild-v2',
            'exported_components': []
        }
        
        try:
            if self.config.include_chat_history:
                chat_data = self._export_chat_history()
                export_manifest['exported_components'].append({
                    'component': 'chat_history',
                    'records_count': len(chat_data),
                    'file': 'chat_history.json'
                })
            
            if self.config.include_preferences:
                pref_data = self._export_preferences()
                export_manifest['exported_components'].append({
                    'component': 'preferences',
                    'records_count': len(pref_data),
                    'file': 'preferences.json'
                })
            
            if self.config.include_workspaces:
                workspace_data = self._export_workspaces()
                export_manifest['exported_components'].append({
                    'component': 'workspaces',
                    'records_count': len(workspace_data),
                    'file': 'workspaces.json'
                })
            
            if self.config.include_tool_metrics:
                metrics_data = self._export_tool_metrics()
                export_manifest['exported_components'].append({
                    'component': 'tool_metrics',
                    'records_count': len(metrics_data),
                    'file': 'tool_metrics.json'
                })
            
            if self.config.include_system_state:
                state_data = self._export_system_state()
                export_manifest['exported_components'].append({
                    'component': 'system_state',
                    'records_count': len(state_data) if isinstance(state_data, list) else 1,
                    'file': 'system_state.json'
                })
            
            # Save export manifest
            with open(self.export_dir / 'export_manifest.json', 'w') as f:
                json.dump(export_manifest, f, indent=2)
            
            # Create backup of original database
            if os.path.exists(self.config.source_db_path):
                shutil.copy2(
                    self.config.source_db_path,
                    self.export_dir / f'backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db'
                )
            
            # Create archive if requested
            if self.config.compress_output:
                self._create_archive()
            
            self.logger.info("Data export completed successfully")
            return export_manifest
            
        except Exception as e:
            self.logger.error(f"Export failed: {str(e)}")
            raise
    
    def _export_chat_history(self) -> List[Dict[str, Any]]:
        """Export chat history data"""
        self.logger.info("Exporting chat history")
        
        chat_data = []
        try:
            # Check if database exists
            if not os.path.exists(self.config.source_db_path):
                self.logger.warning("Source database not found, creating empty chat history export")
                with open(self.export_dir / 'chat_history.json', 'w') as f:
                    json.dump([], f, indent=2)
                return []
            
            conn = sqlite3.connect(self.config.source_db_path)
            conn.row_factory = sqlite3.Row
            
            # Check if chat tables exist
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '%chat%'")
            tables = cursor.fetchall()
            
            if not tables:
                self.logger.warning("No chat tables found, creating empty export")
                chat_data = []
            else:
                # Export from available chat tables
                for table in tables:
                    table_name = table[0]
                    cursor.execute(f"SELECT * FROM {table_name}")
                    rows = cursor.fetchall()
                    
                    for row in rows:
                        chat_data.append({
                            'table': table_name,
                            'data': dict(row),
                            'exported_at': datetime.utcnow().isoformat()
                        })
            
            conn.close()
            
        except sqlite3.Error as e:
            self.logger.error(f"Database error during chat history export: {str(e)}")
            chat_data = []
        
        # Save chat history
        with open(self.export_dir / 'chat_history.json', 'w') as f:
            json.dump(chat_data, f, indent=2, default=str)
        
        self.logger.info(f"Exported {len(chat_data)} chat history records")
        return chat_data
    
    def _export_preferences(self) -> List[Dict[str, Any]]:
        """Export user preferences"""
        self.logger.info("Exporting preferences")
        
        preferences = []
        try:
            # Look for preferences in various locations
            config_files = [
                Path(self.config.source_db_path).parent / 'config' / 'workspace_config.json',
                Path.cwd() / 'config' / 'workspace_config.json',
                Path.home() / '.ai-rebuild' / 'preferences.json'
            ]
            
            for config_file in config_files:
                if config_file.exists():
                    with open(config_file, 'r') as f:
                        data = json.load(f)
                        preferences.append({
                            'source': str(config_file),
                            'data': data,
                            'exported_at': datetime.utcnow().isoformat()
                        })
            
            # If no config files found, create default structure
            if not preferences:
                preferences = [{
                    'source': 'default',
                    'data': {
                        'user_preferences': {},
                        'system_settings': {},
                        'agent_configurations': {}
                    },
                    'exported_at': datetime.utcnow().isoformat()
                }]
                
        except Exception as e:
            self.logger.error(f"Error exporting preferences: {str(e)}")
            preferences = []
        
        # Save preferences
        with open(self.export_dir / 'preferences.json', 'w') as f:
            json.dump(preferences, f, indent=2)
        
        self.logger.info(f"Exported {len(preferences)} preference sets")
        return preferences
    
    def _export_workspaces(self) -> List[Dict[str, Any]]:
        """Export workspace configurations"""
        self.logger.info("Exporting workspaces")
        
        workspaces = []
        try:
            # Scan for workspace-related data
            workspace_dirs = [
                Path.cwd(),
                Path.home() / '.ai-rebuild' / 'workspaces'
            ]
            
            for workspace_dir in workspace_dirs:
                if workspace_dir.exists():
                    for item in workspace_dir.rglob('*workspace*'):
                        if item.is_file() and item.suffix == '.json':
                            try:
                                with open(item, 'r') as f:
                                    data = json.load(f)
                                    workspaces.append({
                                        'source': str(item),
                                        'name': item.stem,
                                        'data': data,
                                        'exported_at': datetime.utcnow().isoformat()
                                    })
                            except Exception as e:
                                self.logger.warning(f"Could not read workspace file {item}: {str(e)}")
            
            # Add current project as workspace
            current_workspace = {
                'source': 'current_project',
                'name': 'ai-rebuild-current',
                'data': {
                    'path': str(Path.cwd()),
                    'type': 'development',
                    'status': 'active'
                },
                'exported_at': datetime.utcnow().isoformat()
            }
            workspaces.append(current_workspace)
            
        except Exception as e:
            self.logger.error(f"Error exporting workspaces: {str(e)}")
            workspaces = []
        
        # Save workspaces
        with open(self.export_dir / 'workspaces.json', 'w') as f:
            json.dump(workspaces, f, indent=2)
        
        self.logger.info(f"Exported {len(workspaces)} workspaces")
        return workspaces
    
    def _export_tool_metrics(self) -> List[Dict[str, Any]]:
        """Export tool usage metrics"""
        self.logger.info("Exporting tool metrics")
        
        metrics = []
        try:
            # Look for metrics in logs and database
            log_dirs = [
                Path.cwd() / 'logs',
                Path.home() / '.ai-rebuild' / 'logs'
            ]
            
            for log_dir in log_dirs:
                if log_dir.exists():
                    for log_file in log_dir.glob('*.log'):
                        # Extract basic metrics from log files
                        metrics.append({
                            'source': str(log_file),
                            'type': 'log_file',
                            'size': log_file.stat().st_size,
                            'modified': datetime.fromtimestamp(log_file.stat().st_mtime).isoformat(),
                            'exported_at': datetime.utcnow().isoformat()
                        })
            
            # Add synthetic metrics for demo
            synthetic_metrics = {
                'source': 'synthetic',
                'type': 'usage_metrics',
                'data': {
                    'total_sessions': 50,
                    'total_tool_calls': 1250,
                    'most_used_tools': ['search_tool', 'code_execution_tool', 'knowledge_search'],
                    'average_session_duration': 300,
                    'success_rate': 0.95
                },
                'exported_at': datetime.utcnow().isoformat()
            }
            metrics.append(synthetic_metrics)
            
        except Exception as e:
            self.logger.error(f"Error exporting tool metrics: {str(e)}")
            metrics = []
        
        # Save metrics
        with open(self.export_dir / 'tool_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        self.logger.info(f"Exported {len(metrics)} metric records")
        return metrics
    
    def _export_system_state(self) -> Dict[str, Any]:
        """Export current system state"""
        self.logger.info("Exporting system state")
        
        system_state = {
            'export_timestamp': datetime.utcnow().isoformat(),
            'python_version': os.sys.version,
            'working_directory': str(Path.cwd()),
            'environment_variables': dict(os.environ),
            'installed_packages': self._get_installed_packages(),
            'system_info': {
                'platform': os.name,
                'cpu_count': os.cpu_count()
            },
            'file_structure': self._get_file_structure()
        }
        
        # Save system state
        with open(self.export_dir / 'system_state.json', 'w') as f:
            json.dump(system_state, f, indent=2, default=str)
        
        self.logger.info("Exported system state")
        return system_state
    
    def _get_installed_packages(self) -> List[str]:
        """Get list of installed packages"""
        try:
            import subprocess
            result = subprocess.run(['pip', 'list', '--format=json'], 
                                  capture_output=True, text=True, check=True)
            return json.loads(result.stdout)
        except Exception:
            return []
    
    def _get_file_structure(self) -> Dict[str, Any]:
        """Get current project file structure"""
        structure = {}
        try:
            for root, dirs, files in os.walk(Path.cwd()):
                # Skip hidden and unnecessary directories
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
                
                rel_root = os.path.relpath(root, Path.cwd())
                if rel_root == '.':
                    rel_root = 'root'
                
                structure[rel_root] = {
                    'directories': dirs,
                    'files': files,
                    'file_count': len(files)
                }
        except Exception as e:
            self.logger.warning(f"Could not generate file structure: {str(e)}")
            structure = {'error': str(e)}
        
        return structure
    
    def _create_archive(self) -> str:
        """Create compressed archive of exported data"""
        self.logger.info("Creating export archive")
        
        import tarfile
        archive_path = f"{self.export_dir}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.tar.gz"
        
        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(self.export_dir, arcname=os.path.basename(self.export_dir))
        
        self.logger.info(f"Created archive: {archive_path}")
        return archive_path

def main():
    """Main entry point"""
    # Default configuration
    config = ExportConfig(
        source_db_path=str(Path.cwd() / 'data' / 'ai_rebuild.db'),
        export_dir=str(Path.cwd() / 'migration_export'),
        include_chat_history=True,
        include_preferences=True,
        include_workspaces=True,
        include_tool_metrics=True,
        include_system_state=True,
        compress_output=True
    )
    
    # Create exporter and run
    exporter = DataExporter(config)
    manifest = exporter.export_all_data()
    
    print("\nData Export Summary:")
    print("=" * 50)
    for component in manifest['exported_components']:
        print(f"â€¢ {component['component']}: {component['records_count']} records")
    
    print(f"\nExport completed successfully!")
    print(f"Export directory: {config.export_dir}")

if __name__ == "__main__":
    main()